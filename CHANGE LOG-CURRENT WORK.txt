[CURENT WORK]

1) Trying to find out why agent isn't learning.

2) The actor actions are changing but only by very small amounts (to five decimal places, 0.99995). Then quickly converges on -1 or 1, something like [-1,1]. I looks like the agent ends up choosing the same action every time even though it's being negativly rewarded for it.

3) running 300 episodes (100 batch size, ~200 warm-up) had no effect.

3) I'm curiouse if the action clipping is hiding details on this.

3) next: trace the choices the agent is making. 

[CHANGE LOG]

[11/5/23] 
1) game.js[line 57]: Added console.info(tf.memory()); 

2) changed all basic numbers into scalers durring math because .mul() and .add() requires tensors.

3) Add creation of deep copies of objects to prevent reference contamination.

[11/6/23] 
1) Added a "dummy" visual representation of where the agent/player starts.

2) Hopefully fixed the updating of all models (applying gradients and so forth). 

3) set target model compile loss to MSE (it doesn't matter from what I understand, because I'm using custom training and loss)

[11/6/23] erased some unused comment code

[11/7/23] re-enabled this.updateTarget(1); at Actor init to set target models to main models

[11/8/23]
1) sample now properly pulls a whole batch and not just a single set. (at least I think that's proper)

2) "start TD3 AI" button re-enables after TD3 script completes.

3) added "distance to civilian" into the observation state

4) changed model units to 400 (L1) and 300 (L2)

5) fixed min_action and max_action not using [0] on assignment

6) created a temporary new branch for the Batch Sample update

[11/9/23]

1) discovered that the line actions.add(noise) wasn't doing the math, so I re-wrote everything like the following because I know it works: actions = tf.add(actions, noise);

2) agent now must break -0.5 to 0.5 threashhold to move (may be temporary we'll see).
