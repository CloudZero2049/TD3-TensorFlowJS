[CURENT WORK]

1) The agent appears to be learning to some degree. A recent run has the agent going back and forth from negative to possitive movement, resulting in a total reward of 1 for the whole step (effectively eliminating bad rewards with good ones). Why doesn't the agent just pick good reward choices? Maybe I need to give him more movement angles.

2) right now, The slower the agent moves towards the civilian the higher his reward (unless he touches the civilian for a bonus)

3) agent is getting rewarded for moving in the general direction of civilian (as long as hes closer than last step). I think I will add the angle as an insentive.

[CHANGE LOG]

[11/5/23] 
1) game.js[line 57]: Added console.info(tf.memory()); 

2) changed all basic numbers into scalers durring math because .mul() and .add() requires tensors.

3) Add creation of deep copies of objects to prevent reference contamination.

[11/6/23] 
1) Added a "dummy" visual representation of where the agent/player starts.

2) Hopefully fixed the updating of all models (applying gradients and so forth). 

3) set target model compile loss to MSE (it doesn't matter from what I understand, because I'm using custom training and loss)

[11/6/23] erased some unused comment code

[11/7/23] re-enabled this.updateTarget(1); at Actor init to set target models to main models

[11/8/23]
1) sample now properly pulls a whole batch and not just a single set. (at least I think that's proper)

2) "start TD3 AI" button re-enables after TD3 script completes.

3) added "distance to civilian" into the observation state

4) changed model units to 400 (L1) and 300 (L2)

5) fixed min_action and max_action not using [0] on assignment

6) created a temporary new branch for the Batch Sample update

[11/9/23]

1) discovered that the line actions.add(noise) wasn't doing the math, so I re-wrote everything like the following because I know it works: actions = tf.add(actions, noise);

2) agent now must break -0.5 to 0.5 threashhold to move (may be temporary we'll see).

[11/10/23]
1) changed the gradient math to use tf.variableGrads.

2) implemented the use of .minimize() for gradient application to models.

3) finaly realized I wasn't normalizing input data. Am now normalizing input data.

[11/11/23]
1) added a "boundry" to agent movement. If it tries to move below 0.001 on x or y axis (top and left sides of map) it is set to 0.001. This should help with normalization since we wont have to deal with negative x,y values.

2) agent must now break a -0.001 to 0.001 threshold in order to move. (affects movement range).

3) agent now moves based on the data stored in action, allowing more range of movement and more direct control.

4) Fixed tf.losses.meanSquaredError(labels, predictions) (they were backwards)

5) removed the zombie x,y coordinates for now because they are static numbers.

6) agent now gets negative points for bumping into boundry walls.

7) now able to set "player/agent" and civillian starting locations and agent goes in the right general direction.

8) added a visual to show the path that the agent chose to take on the canvas. Colors denote batch.

9) removed the civilian x,y coordinates for now because they are static numbers and may have been hampering training.

10) fixed agent x,y only increasing because I needed to use += (not -=) when subtracting.

11) removed ability to start TD3 again after a run (doesn't appear to be working properly)

12) added angle of agent to civillian into observation space (not directly tied to rewards at the moment).

13) tied starting x,y coords to their object counterparts (found in game.js at bottom)
