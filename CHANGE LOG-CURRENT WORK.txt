[CURENT WORK]

1) Good news! I'm proud to announce that after doing a full test of 400 episodes with random spawns for both the agent and civilian, the agent made a perfect run. We now enter an exoerimental phase of fine-tuning (though it may not be necissary).

2) Just before the updates leading to version 0.02 there was a breakdown after about 600 episodes. The hope is that the changes fixed the problem but it still needs testing.

3) I still would like final verification on the full setup of the program by other professionals to ensure there's no loose ends (very dificult without help). At least it is working as expected.

4) I encountered a strange bug were not all models were downloaded (specifically critic_target2). It is a simple code and I'm not sure what caused it, probably a browser glitch. I was Able to manualy download through the dev console using: await agent.critic_target2.model.save('downloads://critic_target2-model');


[CHANGE LOG]

[11/5/23] 
1) game.js[line 57]: Added console.info(tf.memory()); 

2) changed all basic numbers into scalers durring math because .mul() and .add() requires tensors.

3) Add creation of deep copies of objects to prevent reference contamination.

[11/6/23] 
1) Added a "dummy" visual representation of where the agent/player starts.

2) Hopefully fixed the updating of all models (applying gradients and so forth). 

3) set target model compile loss to MSE (it doesn't matter from what I understand, because I'm using custom training and loss)

[11/6/23] erased some unused comment code

[11/7/23] re-enabled this.updateTarget(1); at Actor init to set target models to main models

[11/8/23]
1) sample now properly pulls a whole batch and not just a single set. (at least I think that's proper)

2) "start TD3 AI" button re-enables after TD3 script completes.

3) added "distance to civilian" into the observation state

4) changed model units to 400 (L1) and 300 (L2)

5) fixed min_action and max_action not using [0] on assignment

6) created a temporary new branch for the Batch Sample update

[11/9/23]

1) discovered that the line actions.add(noise) wasn't doing the math, so I re-wrote everything like the following because I know it works: actions = tf.add(actions, noise);

2) agent now must break -0.5 to 0.5 threashhold to move (may be temporary we'll see).

[11/10/23]
1) changed the gradient math to use tf.variableGrads.

2) implemented the use of .minimize() for gradient application to models.

3) finaly realized I wasn't normalizing input data. Am now normalizing input data.

[11/11/23]
1) added a "boundry" to agent movement. If it tries to move below 0.001 on x or y axis (top and left sides of map) it is set to 0.001. This should help with normalization since we wont have to deal with negative x,y values.

2) agent must now break a -0.001 to 0.001 threshold in order to move. (affects movement range).

3) agent now moves based on the data stored in action, allowing more range of movement and more direct control. (toFixed(5) decimal places)

4) Fixed tf.losses.meanSquaredError(labels, predictions) (they were backwards)

5) removed the zombie x,y coordinates for now because they are static numbers.

6) agent now gets negative points for bumping into boundry walls.

7) now able to move player and civillian and player goes in the right general direction.

8) added a visual to show the path that the agent chose to take on the canvas.

9) removed the civilian x,y coordinates for now because they are static numbers.

10) fixed agent x,y only increasing because I needed to use += (not -=) when subtracting.

11) removed ability to start TD3 again after a run (doesn't appear to be working properly)

12) added angle of agent to civillian into observation space (not directly tied to rewards at the moment).

13) tied starting x,y coords to their object counterparts (found in game.js)

14) Implemented a new reward system combining agent distance from civilian and angle, being rewarded more for a better angle and lower distance.

[11/12/23]

1) all coordinates are now based on centers of the "characters" instead of their top/left x,y postions.

2) change collision detection (at least for now) to rely on the distance of player to civilian (simplifying the process). Zombie will not have collision for now.

3) added a seperate "maxStepCount" so that step length isn't determined by batch size.

4) added html sliders to set agent and civilian positions (script on index.html).

5) changes to reward system (currently working on this)

6) changed movement clipping to -0.1 and 0.1

7) created a "sit penalty" to help prevent agent from sitting still to collect angle reward.

8) re-enabled "Start TD3 AI" button after run. resets shown agent paths.

9) added checkbox to keep displayed agent paths [checked], or to erase them befor each run.

[11/13/23]

1) added player x,y speed to observation space.

2) re-added civilian x,y coordinates to observation space.

3) increased layer units back up to 512

4) agent movement reward is now based on each x and y change individually instead of lumping them together.

5) agent "player" and civilian can now be moved after each run.

6) able to set episodes, steps, batch size, and warmup steps before the FIRST run. steps, batch size, and warmup will be disabled after the first run.

[11,14,23]

1) added a "currentStep" variable to Agent class to be used with reward timers.

2) created a UI JS object for sliders (game.js).

3) actually added agent speed updates to observation space

4) included a Game.maxDrawEpisodes variable to limit how many agent paths are drawn (older paths get spliced).

5) more work on reward function. I removed the normalization of the final value for now as it seems to improve the agent drasticaly. However, this does mean I must be extra carefull with the values of the reward fucntion.

6) removed sitPenalty because I'm using a time-based (step-based) penalty system instead.

7) added the agents movement direction to observation space

8) created a locNums object in the observation space so I can update location numbers at a single location. (ex. in envStep LN.AX, LN.dist, etc)

[11/15/23]

1) removed rewardarding x,y movements seperately (can lead to going in wrong direction)

2) changed agent move direction x and y to simply be -1, 0, or 1.

3) fixed drawing path offset

4) turned the angle reward mechanic into a penalty.

5) changed movement threshold to 0.1. I'm trying to help prevent sticking right angles. x

6) agent alpha learning rate lowered to 0.0001. (may have worked well, keeping for now).

[11/16/23]

1) setup a random spawn mechanic for the player (within a fixed radius of civilian). The idea is to train to to find the civilian from any starting angle.

2) Agent Wins are now shown on canvas.

[11/17/23]

1) Created a system to download and load memory buffers (cnt, maxsize, state_memory, next_state_memory, action_memory, reward_memory, done_memory)

[11/19/23]

1) Added a version number to Game object (game.js). Right now it is primarily to check uploaded model compatibility.

2) removed "rewards" and "punishments" text.

3) added a "terminalFlag" to agent moves for drawing.

4) fixed alpha transparency on drawn agent paths.

[11/20/23]

1) Completed download / upload model mechanic. You can download just the actor_main model or all the models, and you can upload multiple models at once (may need some testing).

2) try/catch statements have been set up for downloading and loading models for custom error messages (and to prevent script crashing).

3) To prevent errors, you can only upload saved models before running the agent for the first time.

[11/21/23]

1) Changed page title to "TD3 AI: Find Target"

2) Removed zombie from the canvas (for now)

3) Added an "episodesRan" variable (game.js) to keep track of episodes ran (since page refresh).

4) created a "civilianMoves" variable (game.js) to keep track of random civilian spawns for drawing.(coords are pushed at envReset()). Civilian locations are represented as an ring tied to the color of the agent episode.

5) fixed agent random spawn distance. Max spawn distance is now player.speed * episode length - controll value. Minimum is 5 steps to reach civilian.

6) updated to version 0.01

7) player (agent) speed (in pixels) is now tied directly to the player object (player.speed);

8) fixed not taking absolute value of action[2] and action[3] for movement math. (these only determin speed not direction). This was mainly affecting the agent's colision with walls.

9) replaced hard-coded distance numbers in wall colision detection with (player.width/2) and (player.height/2);

10) adjusted random spawn checking to be more accurate.

11) When agent doesn't move 0.5 or -0.5 is now sent to observation space instead of 0 (-1 or 1 for actual movement). Helps prevent zeroing out.

12) Now doing custom normalization for every input entry. Multiplying x,y coordinates by 0.002 (observationSpace.xyNorm). 500 = 1. Distance is also normalized using xyNorm. Angle is normalized from 0-1.

13) Adding a 2 new arrays to observation space that keep un-normalized location data, agentCoords and civCoords.

14) adding 2 new variables in observation space to eep un-normalized distance and angle, rawDist and rawAngle.

15) changed canvas width to 500 for easier normalization of coordinates.

[11/22/23]

1) movement threshold slightlity increased to 0.15

2) I'm removing the actions from the observation states because I'm led to believe that they might interfere with actor training (and could possibly cause feedback loops). The actions are of course still included when sent to the critic models. The other variables such as agent coordinates, distance, and angle will still be included because they are influenced by but do not come directly from the actions.

3) finished doing custom normalization on state factors. Total reward per step will be between -1 and 0 unless agent reaches the civilian in which case reward is 1. The penalties for distance and angle combine to be between 0-1 (they are subtracted at the end but before the civilian-reached check), and although theres a penalty of 0.5 for touching a wall and a continuous time penalty incrued, the max penalty is clipped to 1 (-1 reward).

4) I'm proud to announce that after doing a full test of 400 episodes with random spawns for both the agent and civilian, the agent made a perfect run. We now enter an exoerimental phase of fine-tuning (though it may not be necissary).

5) Increased version to 0.02

-------------------------------------------------
[To do notes]

civilian circle indicator colors may not be working 100% as intended.
is leakyRelu better than Relu?
