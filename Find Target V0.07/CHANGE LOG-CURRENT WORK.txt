[CURENT WORK] Version 0.07 . Find Target V7

1) Began with updated V-0.06 (first was V-0.03)


[CHANGE LOG]

[1/6/24]

1) removed distance from observation space
2) removed all coords except the agent's from observation space (relying on the rays now).

[1/15/24]

3) removed rays detecting walls.

4) increased memory buffer from 100k to 200k

5) Lowered the hidden layer node count to 32 (down from 512...yah). This appears to have worked very well.

[1/19/24]

1) Added L2 kernel regularizers to models.

2) Increaseing version to 0.07

[1/23/24]

1) reduced movement threshold from 0.15 to 0.

[1/24/24]

1) reduced tau from 0.005 to 0.0005

2) hidden layer neuron count is set to 128 per layer.

[1/30/24]

1) create a logLosses() function in utilsAI to table log the most recent loss values at end of all episodes.

2) created a "decScale" variable within the Agent class that is used for clipping decimal values.

3) rewards and penalties are only given if a sensor detects something. This might be risky but the alternative is the agen't being rewarded or penalized without feedback from sensors.

4) increased sensor count to 45 up from 35. I'm doing this to reduce the chance of sensors missing the civilian when further away.

[1/31/24]

1) learning rates set to 0.0001, 0.0002, and 0.0005 (critic, actor, tau)

2) increased max_memory size to 500,000

[2/7/24]

1) Started finetuning a model based on 128 hidden nodes. Setting L2 to 0.1 had the biggest impact for better performance.

2) Finetuned reward function

3) Finally, 99-100% sucess rate models!

4) updating version to 0.08

[Random Notes]

is leakyRelu better than Relu?
